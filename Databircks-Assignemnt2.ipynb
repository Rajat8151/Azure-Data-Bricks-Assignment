{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df7e83ba-34cd-4f9e-8fb5-eb5ec719cf28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<span style=\"color:orange\">\n",
    " <h2> Databircks-Assignemnt 2 (Basic Data Cleaning Transformation)\n",
    "</span>\n",
    "  <h5>\n",
    "    <span style=\"color:red\">\n",
    "<b>Author: Deepak Goyal <br>\n",
    "   <a> adeus.azurelib.com </a><br>\n",
    "   Email at: admin@azurelib.com\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required data\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('Practise').getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df= spark.read.csv(\"EmployeeData-Assignment 2.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- creationDate: timestamp (nullable = true)\n",
      " |-- bonus: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------------------+------+------+-------------------+--------+\n",
      "| id|first_name|last_name|               email|gender|salary|       creationDate|   bonus|\n",
      "+---+----------+---------+--------------------+------+------+-------------------+--------+\n",
      "|  1|    Valene|   Ingley|vingley0@livejour...|Female| 44104|1957-09-09 16:44:40|    NULL|\n",
      "|  2|  Lynnelle|    Hurll| lhurll1@answers.com|Female|112411|1907-05-10 17:38:56|    NULL|\n",
      "|  3|   Miranda|    Train|   mtrain2@imgur.com|Female| 91073|1941-01-24 16:05:23|12875.54|\n",
      "|  4|    Dulsea|     Foss|dfoss3@dagondesig...|Female|193291|1942-05-09 20:59:39|    NULL|\n",
      "|  5|    Anatol|  Dunklee| adunklee4@google.de|  Male| 22175|1950-07-26 16:28:00| 1432.12|\n",
      "+---+----------+---------+--------------------+------+------+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62c53c84-5d19-4115-a775-314575ce42fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b> Find the count of duplicate employee records in the input file (based on id)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d4e6162-dd1d-497a-b403-121b7b3ad8da",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of duplicate employee records in the input file (based on id): 971\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "dup_emp = df.drop_duplicates(['id']).count()\n",
    "print('Count of duplicate employee records in the input file (based on id):',dup_emp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8364e840-c519-4ac5-a1bf-b9b89b994321",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b> Find out how many records have Gender value missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0185598-138a-4fd1-a503-3e1a0c301712",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records have Gender value missing: 28\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "mis_gender = df.filter(df.gender.isNull()).count()\n",
    "print('Records have Gender value missing:',mis_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14772c70-beff-49d9-828a-724188863639",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "d\n",
    "<b> Are there any missing values in the \"bonus\" field? If so, filled them defualt bonus 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonus missing null values count 0\n"
     ]
    }
   ],
   "source": [
    "print('Bonus missing null values count',df.filter(df.bonus.isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5233f351-ed0d-4c5d-8c49-b382053a6e91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code using the Spark Function\n",
    "mis_bonus = df.filter(df.bonus.isNull()).count()\n",
    "if mis_bonus>0:\n",
    "    df = df.fillna(100,'bonus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonus updated null values count 0\n"
     ]
    }
   ],
   "source": [
    "print('Bonus updated null values count',df.filter(df.bonus.isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cae0fe03-6339-4dac-9a18-1a5a1da78612",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "d\n",
    "<b> Are there any employees with negative salary or bonus amounts in the input file? If so, how many?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bf0728d-c9ff-4d5f-9cb4-00d7a34bb988",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+-----+------+------+------------+-----+\n",
      "| id|first_name|last_name|email|gender|salary|creationDate|bonus|\n",
      "+---+----------+---------+-----+------+------+------------+-----+\n",
      "+---+----------+---------+-----+------+------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "df.filter((df.salary<0)|(df.bonus<0)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e58642b-6aff-4351-92a0-cea48bfdc948",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "d\n",
    "<b> Replace all the null/emtpy value in email column with admin@azurelib.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.email.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4288320d-f41e-4719-99a6-93fe790beacb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code using the Spark Function\n",
    "df = df.fillna('admin@azurelib.com',['email'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.email.isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d39815e9-f12a-497e-a658-fc9f1c98d271",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "d\n",
    "<b> Remove all the records where any record has any null values. Find out the total count of the records now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Null values in table before dropping: 365\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in df.columns:\n",
    "    c = df.filter(col(i).isNull()).count()\n",
    "    count = count+c\n",
    "print('Total number of Null values in table before dropping:',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1035982-814b-43ff-b9df-55e89c658f68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code using the Spark Function\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Null values in table After dropping: 0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in df.columns:\n",
    "    c = df.filter(col(i).isNull()).count()\n",
    "    count = count+c\n",
    "print('Total number of Null values in table After dropping:',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'first_name',\n",
       " 'last_name',\n",
       " 'email',\n",
       " 'gender',\n",
       " 'salary',\n",
       " 'creationDate',\n",
       " 'bonus']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------+--------------------+-------+-------+-------------------+--------+\n",
      "|id1|first_name1|last_name1|              email1|gender1|salary1|      creationDate1|  bonus1|\n",
      "+---+-----------+----------+--------------------+-------+-------+-------------------+--------+\n",
      "|  1|     Valene|    Ingley|vingley0@livejour...| Female|  44104|1957-09-09 16:44:40|   100.0|\n",
      "|  2|   Lynnelle|     Hurll| lhurll1@answers.com| Female| 112411|1907-05-10 17:38:56|   100.0|\n",
      "|  3|    Miranda|     Train|   mtrain2@imgur.com| Female|  91073|1941-01-24 16:05:23|12875.54|\n",
      "|  4|     Dulsea|      Foss|dfoss3@dagondesig...| Female| 193291|1942-05-09 20:59:39|   100.0|\n",
      "|  5|     Anatol|   Dunklee| adunklee4@google.de|   Male|  22175|1950-07-26 16:28:00| 1432.12|\n",
      "|  6|      Baily|    Antony| bantony5@sfgate.com|   Male| 127337|1913-10-14 11:25:33|   100.0|\n",
      "|  7|     Eunice|    Cardus|ecardus6@scientif...| Female| 136574|1901-11-02 15:09:47|38676.94|\n",
      "|  8|   Aubrette|   Lippett| alippett7@nifty.com| Female| 165713|1919-12-16 08:20:42|60150.66|\n",
      "|  9|    Sibylla| Sickamore|ssickamore8@faceb...| Female| 107243|2020-03-15 15:00:30|   100.0|\n",
      "| 11|       Coop|     Richt|   crichta@sogou.com|   Male|  19964|1939-02-13 16:18:24|56973.41|\n",
      "| 12|   Giuseppe|   Scimoni|gscimonib@craigsl...|   Male| 176531|1967-04-06 07:23:03|   100.0|\n",
      "| 13|     Lovell|   Iorizzo|liorizzoc@cpanel.net|   Male| 189150|1949-06-06 16:25:20| 41474.8|\n",
      "| 14|        Deb|     Mogra|   dmograd@bbc.co.uk| Female| 197829|1997-07-27 14:55:52|46528.57|\n",
      "| 15|   Hastings|  Jelliman|hjellimane@histat...|   Male| 179296|1915-07-05 07:02:06|80194.64|\n",
      "| 17|      Gilly|    Fownes|gfownesg@redcross...| Female|   2527|1946-07-30 21:10:57|   100.0|\n",
      "| 19|      Maris| Chatelain| mchatelaini@unc.edu| Female| 190047|2008-01-11 17:16:21|76469.65|\n",
      "| 20|     Casper|   Aughtie|    caughtiej@vk.com|   Male|  38689|1982-04-06 05:13:49| 80404.8|\n",
      "| 21|     Kristo|   Giraudy|  admin@azurelib.com|   Male| 152116|1962-08-16 21:27:30| 5982.54|\n",
      "| 22|      Becki|     Conry|bconryl@examiner.com| Female|  10203|2003-02-07 10:00:05|47067.23|\n",
      "| 23|       Edie|     Orniz|eornizm@moonfruit...| Female|  81920|1982-10-03 14:48:25|20019.85|\n",
      "+---+-----------+----------+--------------------+-------+-------+-------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_col = df.columns\n",
    "new = ['id1',\n",
    " 'first_name1',\n",
    " 'last_name1',\n",
    " 'email1',\n",
    " 'gender1',\n",
    " 'salary1',\n",
    " 'creationDate1',\n",
    " 'bonus1']\n",
    "\n",
    "for i in range(len(old_col)):\n",
    "    df = df.withColumnRenamed(old_col[i],new[i])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Databircks-Assignemnt2 (Basic Data Cleaning Transformation)",
   "notebookOrigID": 534745604295369,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
