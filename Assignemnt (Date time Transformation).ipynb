{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6987a7a6-3052-4bf1-870d-06369bbbc824",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<span style=\"color:orange\">\n",
    " <h2> Databircks-Assignemnt (Bonus-1)  (Date time Transformation)\n",
    "</span>\n",
    "  <h5>\n",
    "    <span style=\"color:red\">\n",
    "<b>Author: Deepak Goyal <br>\n",
    "   <a> adeus.azurelib.com </a><br>\n",
    "   Email at: admin@azurelib.com\n",
    "</span>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "481ac1e5-5d14-4b45-be93-b3e62bff919d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<span style=\"color:red\">\n",
    "<b>Input Data set is attached in the assignment get the file from there.<br>\n",
    "   \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required data\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('Practise').getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= spark.read.csv(\"server_logs.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+---------+----------------+---------------+\n",
      "|log_id|          timestamp|      url|response_time_ms|     user_agent|\n",
      "+------+-------------------+---------+----------------+---------------+\n",
      "|   001|2022-04-21T08:31:31|    /home|              68|    Chrome/91.0|\n",
      "|   002|2022-08-02T18:08:34|/products|             137|    Chrome/91.0|\n",
      "|   003|2022-09-04T15:13:31|     /faq|             193|    Chrome/91.0|\n",
      "|   004|2022-12-26T21:20:29|    /home|             120|    Mozilla/5.0|\n",
      "|   005|2022-05-13T00:24:33|   /login|              86|Safari/605.1.15|\n",
      "+------+-------------------+---------+----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- log_id: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- response_time_ms: string (nullable = true)\n",
      " |-- user_agent: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b3ac98b-ae08-42ca-83aa-2c68621fd1c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b>Convert timestamp from string to DateTime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab3fead4-1dbe-4381-8bdf-5cec62470790",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- log_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- response_time_ms: string (nullable = true)\n",
      " |-- user_agent: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "df1 = df.withColumn(\"timestamp\", to_timestamp(df[\"timestamp\"], 'yyyy-MM-dd\\'T\\'HH:mm:ss'))\n",
    "\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ceaa11f-bcb6-4659-a6bc-d4c4983acfb2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b> Extract the date from timestamp and add it as a new column date?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+---------+----------------+---------------+----------+\n",
      "|log_id|          timestamp|      url|response_time_ms|     user_agent|      Date|\n",
      "+------+-------------------+---------+----------------+---------------+----------+\n",
      "|   001|2022-04-21 08:31:31|    /home|              68|    Chrome/91.0|2022-04-21|\n",
      "|   002|2022-08-02 18:08:34|/products|             137|    Chrome/91.0|2022-08-02|\n",
      "|   003|2022-09-04 15:13:31|     /faq|             193|    Chrome/91.0|2022-09-04|\n",
      "|   004|2022-12-26 21:20:29|    /home|             120|    Mozilla/5.0|2022-12-26|\n",
      "|   005|2022-05-13 00:24:33|   /login|              86|Safari/605.1.15|2022-05-13|\n",
      "+------+-------------------+---------+----------------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "df2 = df1.withColumn('Date',to_date(df1.timestamp,'yyyy-MM-dd'))\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "808bdc43-42d2-4945-9bf4-99ff444ee1fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b> Find the number of logs each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d890aac-43dd-49fb-9ac2-999158e327c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|      Date|logs_per_day|\n",
      "+----------+------------+\n",
      "|2022-01-03|           2|\n",
      "|2022-01-04|           1|\n",
      "|2022-01-10|           1|\n",
      "|2022-01-19|           1|\n",
      "|2022-01-20|           1|\n",
      "+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "df3 = df2.groupBy(col('Date')).count()\n",
    "df3 = df3.withColumnRenamed('count','logs_per_day')\n",
    "df3.sort(df3.Date).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5e18b7e-62b5-4410-b160-18f25ae651a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b> Extract hour from timestamp and analyze log count per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "947dd3b0-5b8d-4535-a115-0f459ae8f1ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|hour|count|\n",
      "+----+-----+\n",
      "|  23|    3|\n",
      "|  22|    3|\n",
      "|  21|    4|\n",
      "|  20|    7|\n",
      "|  19|    1|\n",
      "+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "df4 = df2.withColumn('hour',hour(df2.timestamp))\n",
    "log_count_per_hour = df4.groupBy(\"hour\").count().orderBy(\"hour\",ascending=False)\n",
    "log_count_per_hour.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a481d489-9076-491d-9413-afcb2f9bbb1a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b> Identify the busiest hour of the day based on log count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "546e94d4-e4f9-4713-bf37-79b5c9be164c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Busiest Hour: 10 with log count: 10\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "df5 = log_count_per_hour\n",
    "x = df5.sort(col('count').desc()).first()\n",
    "print(\"Busiest Hour:\", x[0], \"with log count:\", x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a0f2ec1-08df-4ebf-937d-f9917e196791",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b> Convert timestamp to a different string format: 'YYYY/MM/DD HH:mm'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          timestamp|\n",
      "+-------------------+\n",
      "|2022-04-21 08:31:31|\n",
      "|2022-08-02 18:08:34|\n",
      "|2022-09-04 15:13:31|\n",
      "|2022-12-26 21:20:29|\n",
      "|2022-05-13 00:24:33|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(df1.timestamp).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65dbcf66-0cea-4387-aa8b-60a88e540f8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+---------+----------------+---------------+----------------+\n",
      "|log_id|          timestamp|      url|response_time_ms|     user_agent|   new_timestamp|\n",
      "+------+-------------------+---------+----------------+---------------+----------------+\n",
      "|   001|2022-04-21 08:31:31|    /home|              68|    Chrome/91.0|2022/04/21 08:31|\n",
      "|   002|2022-08-02 18:08:34|/products|             137|    Chrome/91.0|2022/08/02 18:08|\n",
      "|   003|2022-09-04 15:13:31|     /faq|             193|    Chrome/91.0|2022/09/04 15:13|\n",
      "|   004|2022-12-26 21:20:29|    /home|             120|    Mozilla/5.0|2022/12/26 21:20|\n",
      "|   005|2022-05-13 00:24:33|   /login|              86|Safari/605.1.15|2022/05/13 00:24|\n",
      "+------+-------------------+---------+----------------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "df6 = df1.withColumn('new_timestamp',date_format(to_timestamp(df1.timestamp),'yyyy/MM/dd HH:mm'))\n",
    "df6.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1119178d-1eb5-4308-afa1-4024d6c9e369",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b> Calculate the average response time for each URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63bddabe-f12c-4579-9f2d-e0cfe8e79022",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------------------+\n",
      "|      url|average_response_time_for_each_URL|\n",
      "+---------+----------------------------------+\n",
      "| /contact|                            109.88|\n",
      "|/register|                            112.08|\n",
      "|   /about|                            120.17|\n",
      "|/products|                            124.18|\n",
      "|   /login|                            124.67|\n",
      "|    /blog|                            125.58|\n",
      "|    /home|                            125.73|\n",
      "|     /faq|                             138.0|\n",
      "| /support|                            138.44|\n",
      "|/services|                            149.75|\n",
      "+---------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "df7 = df2.withColumn('response_time_ms',df2.response_time_ms.cast(FloatType()))\n",
    "df7.groupBy(df.url).agg(round(avg(df7.response_time_ms),2).\\\n",
    "                        alias('average_response_time_for_each_URL')).\\\n",
    "                        sort(col('average_response_time_for_each_URL')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a86448f9-436d-42c7-b889-52015d495ed9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b> Filter logs with response time greater than a certain threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc23260b-46d1-4f02-8c4a-a8a8c85e926b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give threshold 50\n",
      "+------+-------------------+---------+----------------+---------------+----------+\n",
      "|log_id|          timestamp|      url|response_time_ms|     user_agent|      Date|\n",
      "+------+-------------------+---------+----------------+---------------+----------+\n",
      "|   001|2022-04-21 08:31:31|    /home|            68.0|    Chrome/91.0|2022-04-21|\n",
      "|   002|2022-08-02 18:08:34|/products|           137.0|    Chrome/91.0|2022-08-02|\n",
      "|   003|2022-09-04 15:13:31|     /faq|           193.0|    Chrome/91.0|2022-09-04|\n",
      "|   004|2022-12-26 21:20:29|    /home|           120.0|    Mozilla/5.0|2022-12-26|\n",
      "|   005|2022-05-13 00:24:33|   /login|            86.0|Safari/605.1.15|2022-05-13|\n",
      "+------+-------------------+---------+----------------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "x = float(input('give threshold '))\n",
    "df8 = df7.filter(df.response_time_ms>x)\n",
    "df8.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f884bb3b-36a4-4ef9-9a89-634eff8ee781",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b> Find the top 5 most visited URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "297aa8ea-7ffa-4db0-90b3-ad73ecb512bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|      url|count|\n",
      "+---------+-----+\n",
      "| /support|   16|\n",
      "|   /about|   12|\n",
      "|    /blog|   12|\n",
      "|/register|   12|\n",
      "|/products|   11|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function# Write your code using the Spark Function\n",
    "df9 = df.groupBy(col('url')).count()\n",
    "df9.orderBy('count',ascending=False).limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c22077b-bf59-4c3d-bc9e-fca18a0873c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b> Calculate the day of the week for each log and count logs per day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8de25cdb-8bee-4652-bebb-73a9549771ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|day_of_week|count|\n",
      "+-----------+-----+\n",
      "|          3|   18|\n",
      "|          2|   17|\n",
      "|          7|   16|\n",
      "|          5|   14|\n",
      "|          4|   14|\n",
      "+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "df2 = df2.withColumn('day_of_week',dayofweek(col('Date')))\n",
    "df2.groupBy(df2.day_of_week).count().orderBy('count',ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a6d437e-7233-44c7-93be-c3f86dfcd55c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b> Identify logs with empty or null timestamp and count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1befcd43-2d94-4a98-a9d6-660c8f301948",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---+----------------+----------+----+-----------+\n",
      "|log_id|timestamp|url|response_time_ms|user_agent|Date|day_of_week|\n",
      "+------+---------+---+----------------+----------+----+-----------+\n",
      "+------+---------+---+----------------+----------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "df10=df2.filter(df2.timestamp.isNull())\n",
    "df10.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "315bf294-cbea-4284-be44-57d5132069f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b> Split user_agent into browser name and version and add as new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+---------+----------------+---------------+----------+-----------+------------+--------+\n",
      "|log_id|          timestamp|      url|response_time_ms|     user_agent|      Date|day_of_week|browser_name| version|\n",
      "+------+-------------------+---------+----------------+---------------+----------+-----------+------------+--------+\n",
      "|   001|2022-04-21 08:31:31|    /home|              68|    Chrome/91.0|2022-04-21|          5|      Chrome|    91.0|\n",
      "|   002|2022-08-02 18:08:34|/products|             137|    Chrome/91.0|2022-08-02|          3|      Chrome|    91.0|\n",
      "|   003|2022-09-04 15:13:31|     /faq|             193|    Chrome/91.0|2022-09-04|          1|      Chrome|    91.0|\n",
      "|   004|2022-12-26 21:20:29|    /home|             120|    Mozilla/5.0|2022-12-26|          2|     Mozilla|     5.0|\n",
      "|   005|2022-05-13 00:24:33|   /login|              86|Safari/605.1.15|2022-05-13|          6|      Safari|605.1.15|\n",
      "+------+-------------------+---------+----------------+---------------+----------+-----------+------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "df2 = df2.withColumn('browser_name',split(df2.user_agent,'/').getItem(0))\\\n",
    "         .withColumn('version',split(df2.user_agent,'/').getItem(1))\n",
    "    \n",
    "#df2 = df2.withColumn('browser_name', split(df2['user_agent'], '/').getItem(0)) \\\n",
    "         #.withColumn('version', split(df2['user_agent'], '/').getItem(1))\n",
    "\n",
    "# Show the DataFrame\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b87cf7d-fde1-44f0-98c5-e84e320677c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b> Find the first and last log entry for each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e790a6b-6394-4ba0-90e2-37141ae90d1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+\n",
      "|      Date|    first_log_entry|     last_log_entry|\n",
      "+----------+-------------------+-------------------+\n",
      "|2022-01-03|2022-01-03 01:50:29|2022-01-03 01:50:29|\n",
      "|2022-01-03|2022-01-03 01:50:29|2022-01-03 16:11:41|\n",
      "|2022-01-04|2022-01-04 06:21:23|2022-01-04 06:21:23|\n",
      "|2022-01-10|2022-01-10 13:04:04|2022-01-10 13:04:04|\n",
      "|2022-01-19|2022-01-19 15:00:05|2022-01-19 15:00:05|\n",
      "+----------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "window = Window.partitionBy('Date').orderBy('timestamp')\n",
    "\n",
    "first_log_entry = min('timestamp').over(window)\n",
    "last_log_entry = max('timestamp').over(window)\n",
    "\n",
    "result =df2.withColumn('first_log_entry', first_log_entry)\\\n",
    "            .withColumn('last_log_entry', last_log_entry)\n",
    "result.select('Date', 'first_log_entry', 'last_log_entry').distinct().orderBy('Date').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48c93dbe-67e5-460e-bab0-804b533ff977",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b> Extract the month from timestamp and analyze log count per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "847bc2e8-bb32-48aa-8fa6-2ab8f55cea80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|month|count|\n",
      "+-----+-----+\n",
      "|    1|    8|\n",
      "|    2|   11|\n",
      "|    3|    5|\n",
      "|    4|    5|\n",
      "|    5|    7|\n",
      "|    6|   12|\n",
      "|    7|   11|\n",
      "|    8|    7|\n",
      "|    9|   10|\n",
      "|   10|    9|\n",
      "|   11|    7|\n",
      "|   12|    8|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "df2 = df2.withColumn('month',month(df2.Date))\n",
    "df2.groupBy(col('month')).count().orderBy('month').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5e9e2a9-4133-407e-bc52-c332538b3a1b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b> Determine the average response time per hour of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn('time',hour(df2.timestamp))\\\n",
    ".withColumn('response_time_ms',df2.response_time_ms.cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f67bd07c-76c9-44d1-9a64-1d7db068fc7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+\n",
      "|time|avg(response_time_ms)|\n",
      "+----+---------------------+\n",
      "|   9|                 70.0|\n",
      "|   4|                 72.0|\n",
      "+----+---------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "df2.groupBy(col('time')).agg(avg(df2.response_time_ms)).orderBy('avg(response_time_ms)').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "625e6480-0d48-428e-8bc5-23c64431af3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b> Create a new column showing the time of day (Morning, Afternoon, Evening, Night) based on timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3296515c-2b05-4f9d-96c9-c9ea595044d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+\n",
      "|time_of_day|time|\n",
      "+-----------+----+\n",
      "|    Morning|   8|\n",
      "|      Night|  18|\n",
      "|  Afternoon|  15|\n",
      "+-----------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "df2 = df2.withColumn('time_of_day',\n",
    "                when((df2.time<=12) & (df2.time>=7) , 'Morning')\n",
    "                .when((df2.time<=17) & (df2.time>12) , 'Afternoon')\n",
    "                .when((df2.time<=1) & (df2.time>17) , 'Evening')\n",
    "                .otherwise('Night'))\n",
    "df2.select(df2.time_of_day,df2.time).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b3c7eb8-65ec-47d1-94e9-80d9e4bc5f1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b> Count the number of logs for each user_agent type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f82b5d00-7e44-4076-8080-826d0c11ffb9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|     user_agent|count|\n",
      "+---------------+-----+\n",
      "|    Mozilla/5.0|   18|\n",
      "|    Chrome/91.0|   31|\n",
      "|  Edge/18.19041|   23|\n",
      "|Safari/605.1.15|   28|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "df2.groupBy([col('user_agent')]).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31cc824e-183d-4cf1-ad6b-71126c3fa92e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b> Find logs where the url contains a specific keyword (e.g., 'contact')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c36e0b45-f426-4870-bb83-8200e4491f43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+--------+----------------+---------------+----------+-----------+------------+--------+-----+----+-----------+\n",
      "|log_id|          timestamp|     url|response_time_ms|     user_agent|      Date|day_of_week|browser_name| version|month|time|time_of_day|\n",
      "+------+-------------------+--------+----------------+---------------+----------+-----------+------------+--------+-----+----+-----------+\n",
      "|   009|2022-10-29 20:47:12|/contact|           177.0|Safari/605.1.15|2022-10-29|          7|      Safari|605.1.15|   10|  20|      Night|\n",
      "|   013|2022-10-20 23:37:44|/contact|           147.0|    Chrome/91.0|2022-10-20|          5|      Chrome|    91.0|   10|  23|      Night|\n",
      "+------+-------------------+--------+----------------+---------------+----------+-----------+------------+--------+-----+----+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "df2.filter(df.url.like('%contact%')).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2a0ab65-4e44-4edd-8781-3c2d9723c9fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b> Convert the timestamp to a Unix timestamp format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d697b9a0-d6e2-425b-a66b-7ee96e56f7f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+---------+----------------+-----------+----------+-----------+------------+-------+-----+----+-----------+--------------+\n",
      "|log_id|          timestamp|      url|response_time_ms| user_agent|      Date|day_of_week|browser_name|version|month|time|time_of_day|unix_timestamp|\n",
      "+------+-------------------+---------+----------------+-----------+----------+-----------+------------+-------+-----+----+-----------+--------------+\n",
      "|   001|2022-04-21 08:31:31|    /home|            68.0|Chrome/91.0|2022-04-21|          5|      Chrome|   91.0|    4|   8|    Morning|    1650510091|\n",
      "|   002|2022-08-02 18:08:34|/products|           137.0|Chrome/91.0|2022-08-02|          3|      Chrome|   91.0|    8|  18|      Night|    1659443914|\n",
      "|   003|2022-09-04 15:13:31|     /faq|           193.0|Chrome/91.0|2022-09-04|          1|      Chrome|   91.0|    9|  15|  Afternoon|    1662284611|\n",
      "+------+-------------------+---------+----------------+-----------+----------+-----------+------------+-------+-----+----+-----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "df2 = df2.withColumn('unix_timestamp',unix_timestamp(df2.timestamp,\"MM-dd-yyyy HH:mm:ss\"))\n",
    "df2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec198c70-feec-41b0-8c9d-510bf76d87ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b> Calculate the time difference in minutes between consecutive logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b484f9ac-b724-4f7a-975f-cebe0fd73201",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------+\n",
      "|          timestamp|     prev_timestamp| time_diff_minutes|\n",
      "+-------------------+-------------------+------------------+\n",
      "|2022-01-03 01:50:29|               NULL|              NULL|\n",
      "|2022-01-03 16:11:41|2022-01-03 01:50:29|             861.2|\n",
      "|2022-01-04 06:21:23|2022-01-03 16:11:41|             849.7|\n",
      "|2022-01-10 13:04:04|2022-01-04 06:21:23| 9042.683333333332|\n",
      "|2022-01-19 15:00:05|2022-01-10 13:04:04|13076.016666666666|\n",
      "+-------------------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "ordered_df = df2.select(df2.timestamp).orderBy('timestamp')\n",
    "lag_window = Window.orderBy('timestamp')\n",
    "df_with_lag = ordered_df.withColumn('prev_timestamp', lag('timestamp').over(lag_window))\n",
    "\n",
    "# Calculate time difference in minutes between consecutive logs\n",
    "df_with_diff = df_with_lag.withColumn('time_diff_minutes', \n",
    "                                      (unix_timestamp('timestamp') - unix_timestamp('prev_timestamp')) / 60)\n",
    "df_with_diff.select('timestamp', 'prev_timestamp', 'time_diff_minutes').show(5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Databircks-Assignemnt Bonus-1  (Date time Transformation)",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
